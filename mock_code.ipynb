{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy . linalg import qr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun (A,b, m =50) :\n",
    "    n = A. shape [1]\n",
    "    x = np.zeros (n)\n",
    "    N = 1 / A. diagonal ()\n",
    "    for k in range (m):\n",
    "        x = x - N * (A @ x - b)\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function that performs an iterative method called \"Jacobi method\" to solve a system of linear equations of the form Ax=b. The input parameters are the matrix A, the vector b, and an optional parameter m, which represents the maximum number of iterations to perform (default value is 50).\n",
    "\n",
    "The function starts by determining the size of the matrix A (number of columns), and initializes a vector x of zeros of the same size. It then computes the reciprocal of the diagonal elements of A and stores them in a vector N.\n",
    "\n",
    "The main loop of the function runs for m iterations, and at each iteration it updates the vector x by subtracting from it the product of N and the difference between A times x and b (i.e., the residual vector). This update step corresponds to the Jacobi method formula.\n",
    "\n",
    "After the loop has completed, the function returns the final estimate of the solution vector x.\n",
    "\n",
    "Note that the Jacobi method is not always the most efficient iterative method for solving systems of linear equations, but it can be useful in some cases, particularly when the matrix A is diagonally dominant.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The purpose of the Jacobi method is to solve a system of linear equations by iteratively updating the solution vector until it converges to the actual solution. The method is iterative and uses diagonal elements of the coefficient matrix to solve for each component of the solution vector. The Jacobi method is a commonly used algorithm in numerical linear algebra and is used in a variety of applications, including physics, engineering, and computer science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun (A, m =50) :\n",
    "    n = A. shape [1]\n",
    "    x = np . zeros (n)\n",
    "    x [0] = 1\n",
    "    for k in range (m):\n",
    "        x = A @ x\n",
    "        x = x / np . linalg . norm (x)\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in a matrix A and an optional integer m (with a default value of 50). It first calculates the number of columns in A and initializes a vector x of the same length with the first element set to 1. Then, for each of the m iterations, it multiplies the matrix A with the vector x and normalizes the resulting vector to have a Euclidean norm of 1. Finally, it returns the resulting vector x.\n",
    "\n",
    "This function appears to be implementing a power iteration algorithm to find the dominant eigenvector of the matrix A. The power iteration algorithm is an iterative method for approximating the eigenvector corresponding to the largest eigenvalue of a matrix. It is a simple and effective algorithm, but may not work well for matrices with multiple eigenvalues of equal magnitude, or for matrices that are not diagonalizable.\n",
    "\n",
    "\n",
    "purpose:\n",
    "\n",
    "The power iteration algorithm is used to find the dominant eigenvector of a square matrix. The dominant eigenvector is the eigenvector corresponding to the eigenvalue with the largest absolute value. It is commonly used in various applications, including data analysis, machine learning, and signal processing.\n",
    "\n",
    "The main purpose of the power iteration algorithm is to efficiently and iteratively calculate the dominant eigenvector of a given matrix. This algorithm works by repeatedly multiplying the matrix with a vector and normalizing the result until convergence is achieved. The resulting vector is an approximation of the dominant eigenvector, which can be used to analyze the properties of the matrix and its corresponding system.\n",
    "\n",
    "Some of the common applications of the power iteration algorithm include principal component analysis, ranking algorithms, and recommendation systems. It is a simple and effective method for analyzing large data sets and identifying the most significant features or factors that contribute to the overall system performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun (A, m =50) :\n",
    "    from scipy . linalg import qr\n",
    "    for k in range (m):\n",
    "        Q, R = qr(A)\n",
    "        A = R @ Q\n",
    "    return A. diagonal ()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in a matrix A and an optional parameter m which defaults to 50. It then applies the QR decomposition to A m times, updating A with the result of each decomposition. Finally, it returns the diagonal elements of the resulting matrix A.\n",
    "\n",
    "The QR decomposition factorizes a matrix A into the product of an orthogonal matrix Q and an upper triangular matrix R. The algorithm repeatedly applies this factorization to update A, with the hope that the upper triangular matrix R converges to a diagonal matrix. This is a useful technique for finding the eigenvalues of a matrix, which are the diagonal elements of the final upper triangular matrix.\n",
    "\n",
    "The function is somewhat limited in that it only returns the diagonal elements, and does not provide any information about the eigenvectors or their corresponding eigenvalues. Additionally, the convergence of the algorithm is not guaranteed, so the function may not always accurately compute the eigenvalues.\n",
    "\n",
    "Which algorithm is implemented?\n",
    "This is an implementation of the QR algorithm for finding the eigenvalues of a matrix. The QR decomposition is used in each iteration to transform the matrix into upper-triangular form, which makes it easier to identify the eigenvalues on the diagonal. The process is repeated multiple times (controlled by the optional parameter \"m\") to improve the accuracy of the results.\n",
    "\n",
    "\n",
    "What is the purpose of this algorithm?\n",
    "The purpose of this algorithm is to compute the eigenvalues of a matrix A using the QR algorithm, which is an iterative algorithm that transforms the matrix into a similar upper-triangular matrix with the same eigenvalues. The algorithm performs a fixed number of iterations (m=50 by default) to converge to the eigenvalues, which are then returned as the diagonal elements of the upper-triangular matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(A, b, theta=.1, m=50):\n",
    " n = A.shape[1]\n",
    " x = np.zeros(n)\n",
    " for k in range(m):\n",
    "   x = x - theta * (A @ x - b)\n",
    " return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in three arguments:\n",
    "\n",
    "A: a numpy array representing the matrix A of a linear system of equations.\n",
    "b: a numpy array representing the vector b of the linear system of equations.\n",
    "theta: a float representing the step size for gradient descent. The default value is 0.1.\n",
    "m: an integer representing the number of iterations for gradient descent. The default value is 50.\n",
    "The function initializes the vector x to a numpy array of zeros. Then, it runs the gradient descent algorithm m times, where at each iteration, it updates the vector x by subtracting the product of A and x from b and multiplying the result by theta. Finally, the function returns the resulting vector x.\n",
    "\n",
    "Overall, this function solves a linear system of equations using the gradient descent algorithm. The algorithm iteratively updates the vector x until it converges to a solution that satisfies the equation Ax = b.\n",
    "\n",
    "\n",
    "\n",
    "This is the gradient descent algorithm for solving a linear system of equations, where A is the coefficient matrix and b is the vector of constants.\n",
    "\n",
    "\n",
    "\n",
    "This algorithm aims to solve a linear system of equations using gradient descent. It takes in a matrix A and a vector b as inputs, and iteratively updates an initial guess for the solution x using the formula x = x - theta * (A @ x - b), where theta is a learning rate parameter and m is the number of iterations. The algorithm aims to minimize the residual error between the matrix product A @ x and the vector b, which corresponds to the difference between the left-hand and right-hand sides of the linear system. The output is the final estimate for x that minimizes the residual error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
